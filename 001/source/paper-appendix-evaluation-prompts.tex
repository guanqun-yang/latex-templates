\section{Prompts}
In this section, we list all the prompts we use throughout the paper. Other than ensuring that generations could be parsed properly, all prompts were not optimized towards any particular models.

\subsection{Benchmark Generation Few-Shot Prompts}
\begin{lstlisting}[caption={All few-shot examples used for benchmark generation},label={lst:benchmark-generation-fewshot}, captionpos=t, breaklines=true]
string_1 = """[TASK]
str.split
[/TASK]
[PYTHON]
def f(text):
    words = text.split()
    result = []
    for i in range(len(words)):
        if i % 2 == 0:
            result.append(words[i][::-1])
        else:
            result.append(words[i].upper())
    return ' '.join(result)
[/PYTHON]
[TEST]
assert f("am7 fiDfd n") == ??
assert f("bnasadl") == ??
assert f("a j c n x X k") == ??
assert f("98 bask2 asoijdf9") = ??
assert f("") == ??
[/TEST]"""

string_2 = """[TASK]
str.capitalize
[/TASK]
[PYTHON]
def f(text):
    a = []
    words = text.split(' ')
    for i in range(len(words)):
        if words[i][0].isdigit():
            return 'no'
        if i%2 == 0:
            a.append(words[i].capitalize())
        else:
            a.append(words[i])
    return ' '.join(a)
[/PYTHON]
[TEST]
assert f("20xk flkawhf") == ??
assert f("lkw hj sfaibw fi 9") == ??
assert f("abbot 2929 mbpu") == ??
assert f("rotor zisxrs fh29nx") == ??
assert f("pxk 5 bxD 9") == ??
[/TEST]"""

string_3 = """[TASK]
str.rindex
[/TASK]
[PYTHON]
def f(text, char):
    index = text.rindex(char)
    result = list(text)
    while index > 0:
        result[index] = result[index-1]
        result[index-1] = char
        index -= 2
    return ''.join(result)
[/PYTHON]
[TEST]
assert f('mnjs krupa', 'u') == ??
assert f('kqwomn0xj', 'm') == ??
assert f('qpfi jzm', 'j') == ??
assert f('102x0zoq', '0') == ??
assert f('nzu  ei,', 'e') == ??
[/TEST]"""

string_4 = """[TASK]
str.rpartition
[/TASK]
[PYTHON]
def f(text, char):
    if char in text:
        pref, char, suff = text.rpartition(char)
        suff = suff[:-len(char)] + char + suff[len(char):]
        return suff + pref
    return text
[/PYTHON]
[TEST]
assert f('smswfwe-r', '-') == ??
assert f(',wpzpppdl/', 'p') == ??
assert f('9284701', '2') == ??
assert f('nvizoh2ja', 'c') == ??
assert f('aaa0a1', 'a') == ??
[/TEST]"""

string_5 = """[TASK]
str.center
[/TASK]
[PYTHON]
def f(text):
    ls = list(text)
    for i in range(1, len(ls) - 1):
        ls.insert(i, '+')
    return ''.join(ls).center((len(ls) - 1) * 2)
[/PYTHON]
[TEST]
assert f('lynel') == ??
assert f('nzoh') == ??
assert f('u') == ??
assert f('anfsoixz') == ??
assert f('xzd') == ??
[/TEST]"""

list_1 = """[TASK]
list.pop
[/TASK]
[PYTHON]
def f(names, num):
    queue = names
    while len(queue) > 1:
        for _ in range(num):
            queue.append(queue.pop(0))
        queue.pop(0)
    return queue.pop()
[/PYTHON]
[TEST]
assert f(['aiwn', 'xke', 'mpwiy'], 2) == ??
assert f(['y', 'z', 'cc', '2', '5', '.', 'zksdfjn'], 7) == ??
assert f(['98bfaj', 'cn11', 'fakldj', 'tjasl', 'a'], 10) == ??
assert f(['aghbvm'], 1) == ??
assert f(['mnv', 'fjw', 'fnk'], 0) == ??
[/TEST]"""

list_2 = """[TASK]
list.insert
[/TASK]
[PYTHON]
def f(text, position, value):
    length = len(text)
    index = position % (length + 1)
    if position < 0 or index < 0:
        index = length // 2
    new_text = list(text)
    new_text.insert(index, value)
    return ''.join(new_text)
[/PYTHON]
[TEST]
assert f('h grateful k', 3, 'h') == ??
assert f('umjwi', -5, 'm') == ??
assert f('coscifysu', 0, 'd') == ??
assert f('fnmart', 4, 'o') == ??
assert f('rzti', -1, 'a') == ??
[/TEST]"""

list_3 = """[TASK]
list.remove
[/TASK]
[PYTHON]
def f(array, elem):
    array.reverse()
    try:
        while elem in array:
            array.remove(elem)
    finally:
        array.reverse()
    return array
[/PYTHON]
[TEST]
assert f([-1, 2, 1, -8, 2], 2) == ??
assert f([], 2) == ??
assert f([1], 1) == ??
assert f([3, 6, 4, -2, 5], 4) == ??
assert f([3, 2, 1, 2, 7, 1], 1) == ??
[/TEST]"""

list_4 = """[TASK]
list.append
[/TASK]
[PYTHON]
def f(nums):
    count = len(nums)
    for i in range(-count+1, 0):
        nums.append(nums[i])
    return nums
[/PYTHON]
[TEST]
assert f([2, 6, 1, 3, 1]) == ??
assert f([7, 1, 2, 6, 0, 2]) == ??
assert f([4, 3, 2, 1, 2, -1, 4, 2]) == ??
assert f([0, 6, 2, -1, -2]) == ??
assert f([-6, -2, 1, -3, 0, 1]) == ??
[/TEST]"""

list_5 = """[TASK]
list.index
[/TASK]
[PYTHON]
def f(nums, swap1, swap2):
    i1 = nums.index(swap1)
    i2 = nums.index(swap2)
    nums[i1], nums[i2], nums[i1 + 1], nums[i2 + 1] = nums[i2], nums[i1], nums[i2 + 1], nums[i1 + 1]
    return nums
[/PYTHON]
[TEST]
assert f([6, 2, 1, 3, 4, 5], 3, 4) == ??
assert f([1, 1, 5, 3, 1, 2], 1, 2) == ??
assert f([1, 2, 1, 4, 1], 4, 2) == ??
assert f([6, 2, 3, 1, 7, 5, 7], 3, 7) == ??
assert f([2, 8, 8, 3, 8, 3, 9], 3, 2) == ??
[/TEST]"""
\end{lstlisting}

\subsection{Direct Prediction Prompts} \label{sec:appendix-direct-prompts}
In Listings \ref{lst:benchmark-direct-input-prompt-llama}, \ref{lst:benchmark-direct-input-prompt-gpt}, \ref{lst:benchmark-direct-output-prompt-llama}, \ref{lst:benchmark-direct-output-prompt-gpt}, and \ref{lst:benchmark-direct-output-prompt-phind}, we include the prompts we use for our evaluation. We use a few-shot prompt for all models other than GPT models. For many models, we observed that using the zero-shot prompt leads to a generation that are not in a easily parsable format, and including the few-shot examples led to predictable formatting. For fairness, we also measured the performance of several few-shot prompts on the GPT models for a randomly sampled subset of the benchmark (instead of the full benchmark for cost reasons). However, we observed a decrease in performance compared to the zero-shot prompts for both input prediction and output prediction. Therefore, we decided to use the zero-shot prompt for GPT models and report numbers using that prompt. In addition, we use a separate output prediction prompt for Phind because the prompt in Listing  \ref{lst:benchmark-direct-output-prompt-llama} often led to explanation text before completing the assert. 

\begin{lstlisting}[caption={Input Prediction (non-GPT)},label={lst:benchmark-direct-input-prompt-llama}, captionpos=t, breaklines=true]
You will be given a function f and an output in the form f(??) == output. Find any input such that executing f on the input leads to the given output. There may be multiple answers, but you should only output one. Think step by step before arriving at an answer. Finally, surround the answer, with no additional words, with [ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will give the output.

[PYTHON]
def f(my_list):
    count = 0
    for i in my_list:
        if len(i) % 2 == 0:
            count += 1
    return count
assert f(??) == 3
[/PYTHON]
[ANSWER]
f(["mq", "px", "zy"])
[/ANSWER]

[PYTHON]
def f(s1, s2):
    return s1 + s2
assert f(??) == "banana"
[/PYTHON]
[ANSWER]
f("ba", "nana")
[/ANSWER]

[PYTHON]
{function}
assert f(??) == {output}
[/PYTHON]
[ANSWER]
\end{lstlisting}

\begin{lstlisting}[caption={Input Prediction (GPT)},label={lst:benchmark-direct-input-prompt-gpt}, captionpos=t, breaklines=true]
You will be given a function f and an output in the form output == f(??). Output the completion of the last line so that the code will run without errors by finding any input such that executing f on the input leads to the given output. There may be multiple answers, and you can output any one. Do NOT output any additional information.

{function}
assert {output} == f(
\end{lstlisting}

\begin{lstlisting}[caption={Output Prediction (non-GPT, non-Phind)},label={lst:benchmark-direct-output-prompt-llama}, captionpos=t, breaklines=true]
Based on the given Python code, which may contain errors, complete the assert statement with the output when executing the code on the given test case. Do NOT output any extra information, even if the function is incorrect or incomplete. Do NOT output a description for the assert.

def f(n):
    return n
assert f(17) == 17

{function}
assert f({input}) ==
\end{lstlisting}

\begin{lstlisting}[caption={Output Prediction (GPT)},label={lst:benchmark-direct-output-prompt-gpt}, captionpos=t, breaklines=true]
Based on the given Python code, which may contain errors, complete the assert statement with the output when executing the code on the given test case. Do not output any extra information, even if the function is incorrect or incomplete.

{function}
assert f({input}) == 
\end{lstlisting}

\begin{lstlisting}[caption={Output Prediction (Phind)},label={lst:benchmark-direct-output-prompt-phind}, captionpos=t, breaklines=true]
Based on the given Python code, which may contain errors, complete the assert statement with the output when executing the code on the given test case. Do NOT output any extra information, even if the function is incorrect or incomplete. Output "# done" after the assertion.

def f(n):
    return n
assert f(17) == 17 # done

{function}
assert f({input}) ==
\end{lstlisting}


\subsection{Chain of Thought Prompts}\label{sec:appendix-cot-prompts}
Below, we include the prompts we use for the chain of thought experiments. For the same reasons mentioned in Appendix \ref{sec:appendix-direct-prompts}, use a one-shot prompt for Code Llama models and a zero-shot prompt for GPT models. 

\begin{lstlisting}[caption={CoT input prediction prompt (Code Llama)},label={lst:benchmark-cot-input-prompt-llama}, captionpos=t, breaklines=true]
You will be given a function f and an output in the form f(??) == output. Your task is to find any input such that executing f on the input leads to the given output. There may be multiple answers, but only output one. First, think step by step. Then, surround the answer with [ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will give the output.

def f(x):
    return x + 1
assert f(??) == 17

To find an input such that executing f on the input leads to the given output, we can work backwards from the given assertion. We know that f(??) == 17. 

Since the function f(x) returns x + 1, for f(??) to be equal to 17, the value of ?? should be 16. 

Therefore, the function call that will give the output as 17 is:
[ANSWER]f(16)[/ANSWER]    

{function}
assert f(??) == {output}
\end{lstlisting}

\begin{lstlisting}[caption={CoT input prediction prompt (GPT)},label={lst:benchmark-cot-input-prompt-gpt}, captionpos=t, breaklines=true]
You will be given a function f and an output in the form f(??) == output. Your task is to find any input such that executing f on the input leads to the given output. There may be multiple answers, but only output one. First, think step by step. Then, surround the answer with [ANSWER] and [/ANSWER] tags. Express your answer as a function call that when executed will give the output.

{function}
assert f(??) == {output}
\end{lstlisting}

\begin{lstlisting}[caption={CoT output prediction prompt (Code Llama)},label={lst:benchmark-cot-output-prompt-llama}, captionpos=t, breaklines=true]
You are given a function and an input. Complete the assertion with the output of executing the function on the input. First, reason step by step before arriving at an answer. Then, surround the answer as an assertion with [ANSWER] and [/ANSWER] tags.

def f(s):
    return s + "a"
assert f("hi") == ??

The function f takes a string s as input and returns the concatenation of s with the string "a".

To determine the output of executing the function f on the input "hi", we need to concatenate "hi" with "a". 

Therefore, the output of executing the function f on the input "hi" is "hia".

[ANSWER]assert f("hi") == "hia"[/ANSWER]

{function}
assert f(input) == ??
\end{lstlisting}

\begin{lstlisting}[caption={CoT output prediction prompt (GPT)},label={lst:benchmark-cot-output-prompt-gpt}, captionpos=t, breaklines=true]
What should the output of this code be so that the assertion is correct? Reason step by step before arriving at an answer. Finally, surround the answer, with no additional words, with [ANSWER] and [/ANSWER] tags.

{function}
\end{lstlisting}